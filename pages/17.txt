<!DOCTYPE html>
<html lang="en">


<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Search Engine Info --><title>Understanding self-supervised learning dynamics without contrastive pairs</title><meta name="description" content="Understanding self-supervised learning dynamics without contrastive pairsYuandong Tian,&nbsp;Xinlei Chen,&nbsp;Surya GanguliWhile contrastive approaches of s..."><!-- Solution from http://stackoverflow.com/questions/31593297/using-execcommand-javascript-to-copy-hidden-text-to-clipboard -->
<script src="https://proceedings.mlr.press/v139/assets/js/copy_input.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://proceedings.mlr.press/v139/assets/js/download.js"></script>
<meta name="citation_publisher" content="PMLR"/><meta name="citation_title" content="Understanding self-supervised learning dynamics without contrastive pairs"/><meta name="citation_language" content="en"/>
<meta name="citation_abstract_html_url" content="https://proceedings.mlr.press/v139/tian21a.html"/><meta name="citation_pdf_url" content="http://proceedings.mlr.press/v139/tian21a/tian21a.pdf"><meta name="citation_firstpage" content="10268"><meta name="citation_lastpage" content="10278"><meta name="citation_author" content="Yuandong Tian"><meta name="citation_author" content="Xinlei Chen"><meta name="citation_author" content="Surya Ganguli"><meta name="citation_publication_date" content="2021/07/01"><meta name="citation_inbook_title" content="International Conference on Machine Learning"/><meta name="citation_conference_title" content="International Conference on Machine Learning"/><meta name="citation_issn" content="2640-3498"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Understanding self-supervised learning dynamics without contrastive pairs"/><meta name="twitter:site" content="@MLResearchPress" /><meta name="twitter:description" content="While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data ..."/><meta name="twitter:image" content="https://proceedings.mlr.press/v139/assets/images/logo-pmlr.png"/>
<meta name="twitter:image:alt" content="PMLR Logo"/><meta property="og:title" content="Understanding self-supervised learning dynamics without contrastive pairs"/>
<meta property="og:description" content="While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing vi..."/>
<meta property="og:type" content="article"/><meta property="article:published_time" content="2021-07-01T00:00:00+00:00"><meta property="og:url" content="https://proceedings.mlr.press/v139/tian21a.html"/><meta property="og:image" content="https://proceedings.mlr.press/v139/assets/images/logo-pmlr.png"/><meta property="og:site_name" content="PMLR"/><!-- Style Info -->
  <link rel="stylesheet" type="text/css" href="https://proceedings.mlr.press/v139/assets/css/pmlr.css" />
  <style>.hero-text {
    color: #303030;
  }</style>

  <!-- Icon Info -->
  <link rel="shortcut icon" href="https://proceedings.mlr.press/v139/assets/images/favicon-pmlr.ico" type="image/x-icon">
  <link rel="icon" href="https://proceedings.mlr.press/v139/assets/images/favicon-pmlr.ico" type="image/x-icon">

  <!-- Feed Info --><link type="application/atom+xml" rel="alternate" href="https://proceedings.mlr.press/v139/feed.xml" title="Proceedings of Machine Learning Research" /><!-- Scripting info --><!-- Solution from http://stackoverflow.com/questions/31593297/using-execcommand-javascript-to-copy-hidden-text-to-clipboard -->
<script src="https://proceedings.mlr.press/v139/assets/js/copy_input.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://proceedings.mlr.press/v139/assets/js/download.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92432422-1', 'auto');
  ga('send', 'pageview');

</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$','$'], ['\\(', '\\)'] ],
      displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
      processEscapes: true,
    }
  });
</script>
<script  async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <!-- User custom header -->
</head>






<body>


<header class="site-header">
  <div class="wrapper">
    


<div class="hero-image">
  <div class="hero-text">
    <a href="/" target="_top"><img src="/v139/assets/images/logo-pmlr.svg" alt="[International Conference on Machine Learning Logo]"></a>
    Proceedings of Machine Learning Research
  </div>
</div>


    <nav class="site-nav">
  <input type="checkbox" id="nav-trigger" class="nav-trigger" />
  <label for="nav-trigger">
  <span class="menu-icon"><svg viewBox="0 0 18 15" width="18px" height="15px">
  <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
</svg>
</span>
  </label>
  <div class="trigger"><a class="page-link" href="https://proceedings.mlr.press/v139/">Volume 139</a><a class="page-link" href="http://www.jmlr.org/">JMLR</a> 
    <a class="page-link" href="http://www.jmlr.org/mloss">MLOSS</a>
    <a class="page-link" href="/faq.html">FAQ</a>
    <a class="page-link" href="/spec.html">Submission Format</a>
    <a class="page-link" href="https://proceedings.mlr.press//v139/assets/rss/feed.xml">
      <img src="https://proceedings.mlr.press/v139/assets/images/RSS.gif" class="rss" alt="RSS Feed">
    </a>
  </div>
</nav>

  </div>
</header>





<main class="page-content" aria-label="Content"><div class="wrapper">


<p style="text-align:right">[<a href="https://github.com/mlresearch/v139/edit/gh-pages/_posts/2021-07-01-tian21a.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/mlresearch/v139/edit/gh-pages/_posts/2021-07-01-tian21a.md', 13);">edit</a>]</p>


<article class="post-content">
  <h1>Understanding self-supervised learning dynamics without contrastive pairs</h1><span class="authors">Yuandong Tian,&nbsp;Xinlei Chen,&nbsp;Surya Ganguli</span>
<div id="info"><i>Proceedings of the 38th International Conference on Machine Learning</i>,&nbsp;PMLR 139:10268-10278,&nbsp;2021.
  </div> <!-- info -->
  
  <h4>Abstract</h4>
  <div id="abstract" class="abstract">
    While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \ourmethod{}, that \emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by $2.5%$ in 300-epoch training (and $5%$ in 60-epoch). \ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.
  </div>
  <h4>Cite this Paper</h4>
<hr class="bibhr">

<div class="bibbuttongroup"><div class="bibbuttontext"> BibTeX</div> <div class="codebox">
  <code class="citecode" id="bibtex">
@InProceedings{pmlr-v139-tian21a,
  title = 	 {Understanding self-supervised learning dynamics without contrastive pairs},
  author =       {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10268--10278},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/tian21a/tian21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/tian21a.html},
  abstract = 	 {While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \ourmethod{}, that \emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by $2.5%$ in 300-epoch training (and $5%$ in 60-epoch). \ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.}
}
</code></div>
<button class="bibbutton"  id="button-bibtex1" onclick="CopyToClipboard('bibtex');  ga('send', 'event', 'Ref Copy', 'CopyToClipboard', 'https://proceedings.mlr.press/v139/pmlr-v139-tian21a.bib', 15);">Copy to Clipboard</button><button class="bibbutton" id="button-bibtex2" onclick="DownloadToFile('pmlr-v139-tian21a.bib', 'bibtex'); ga('send', 'event', 'Ref Downloads', 'Download', 'https://proceedings.mlr.press/v139/pmlr-v139-tian21a.bib', 16);">Download</button></div>


<div class="bibbuttongroup"><div class="bibbuttontext"> Endnote</div> <div class="codebox">
  <code class="citecode" id="endnote">%0 Conference Paper
%T Understanding self-supervised learning dynamics without contrastive pairs
%A Yuandong Tian
%A Xinlei Chen
%A Surya Ganguli
%B Proceedings of the 38th International Conference on Machine Learning
%C Proceedings of Machine Learning Research
%D 2021
%E Marina Meila
%E Tong Zhang	
%F pmlr-v139-tian21a
%I PMLR
%P 10268--10278
%U https://proceedings.mlr.press/v139/tian21a.html
%V 139
%X While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \ourmethod{}, that \emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by $2.5%$ in 300-epoch training (and $5%$ in 60-epoch). \ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.
</code></div>
<button class="bibbutton"  id="button-endnote1" onclick="CopyToClipboard('endnote');  ga('send', 'event', 'Ref Copy', 'CopyToClipboard', 'https://proceedings.mlr.press/v139/pmlr-v139-tian21a.enw', 15);">Copy to Clipboard</button><button class="bibbutton" id="button-endnote2" onclick="DownloadToFile('pmlr-v139-tian21a.enw', 'endnote'); ga('send', 'event', 'Ref Downloads', 'Download', 'https://proceedings.mlr.press/v139/pmlr-v139-tian21a.enw', 16);">Download</button></div>



<div class="bibbuttongroup"><div class="bibbuttontext"> APA</div> <div class="codebox">
  <code class="citecode" id="apa">
Tian, Y., Chen, X. & Ganguli, S.. (2021). Understanding self-supervised learning dynamics without contrastive pairs. <i>Proceedings of the 38th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 139:10268-10278 Available from https://proceedings.mlr.press/v139/tian21a.html.

</code></div>
<button class="bibbutton"  id="button-apa1" onclick="CopyToClipboard('apa');  ga('send', 'event', 'Ref Copy', 'CopyToClipboard', 'https://proceedings.mlr.press/v139/pmlr-v139-tian21a.txt', 15);">Copy to Clipboard</button><button class="bibbutton" id="button-apa2" onclick="DownloadToFile('pmlr-v139-tian21a.txt', 'apa'); ga('send', 'event', 'Ref Downloads', 'Download', 'https://proceedings.mlr.press/v139/pmlr-v139-tian21a.txt', 16);">Download</button></div>

<hr class="bibhr">


  
  <h4>Related Material</h4>
  <div id="extras">
    <ul>
      <li><a href="http://proceedings.mlr.press/v139/tian21a/tian21a.pdf" target="_blank" onclick="ga('send', 'event', 'PDF Downloads', 'Download', 'http://proceedings.mlr.press/v139/tian21a/tian21a.pdf', 10);">Download PDF</a></li>
      <li><a href="http://proceedings.mlr.press/v139/tian21a/tian21a-supp.pdf" target="_blank" onclick="ga('send', 'event', 'Extra Downloads', 'Download', 'http://proceedings.mlr.press/v139/tian21a/tian21a-supp.pdf', 12);">Supplementary PDF</a></li>
    </ul>
  </div>
  

</article>









      </div>
    </main>


<footer class="site-footer">
  <div class="wrapper">
    <p>This site last compiled Mon, 17 Jan 2022 19:06:23 +0000</p>
      <div class="footer-left"><i><a href="https://github.com/mlresearch/v139">Github Account</a></i>
      </div>
      <div class="footer-right">Copyright &copy; <a href="https://proceedings.mlr.press">The authors and PMLR</a> 2022.<a href="https://twitter.com/MLResearchPress"><i class="icon-twitter"></i>&nbsp;MLResearchPress</a></div>
  </div>
</footer>





</body>

</html>
