<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta content="text/html; charset=UTF-8" http-equiv="content-type">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>CVPR 2019 Open Access Repository</title>
<link rel="stylesheet" type="text/css" href="../../static/conf.css">
<script type="text/javascript" src="../../static/jquery.js"></script>
<meta name="citation_title" content="Self-Supervised Learning via Conditional Motion Propagation">
<meta name="citation_author" content="Zhan, Xiaohang">
<meta name="citation_author" content="Pan, Xingang">
<meta name="citation_author" content="Liu, Ziwei">
<meta name="citation_author" content="Lin, Dahua">
<meta name="citation_author" content="Loy, Chen Change">
<meta name="citation_publication_date" content="2019">
<meta name="citation_conference_title" content="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
<meta name="citation_firstpage" content="1881">
<meta name="citation_lastpage" content="1889">
<meta name="citation_pdf_url" content="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_Self-Supervised_Learning_via_Conditional_Motion_Propagation_CVPR_2019_paper.pdf">
</head>
<body>
<div id="header">
<div id="header_left">
<a href="http://cvpr2019.thecvf.com"><img src="../../img/cvpr2019_logo.png" width="175" border="0" alt="CVPR 2019"></a>
<a href="http://www.cv-foundation.org/"><img src="../../img/cropped-cvf-s.jpg" width="175" height="112" border="0" alt="CVF"></a>
</div>
<div id="header_right">
<div id="header_title">
<a href="http://cvpr2019.thecvf.com">CVPR 2019</a> <a href="/" class="a_monochrome">open access</a>
</div>
<div id="help" >
These CVPR 2019 papers are the Open Access versions, provided by the <a href="http://www.cv-foundation.org/">Computer Vision Foundation.</a><br> Except for the watermark, they are identical to the accepted versions; the final published version of the proceedings is available on IEEE Xplore.</div>
<div id="disclaimer" >
This material is presented to ensure timely dissemination of scholarly and technical work.
Copyright and all rights therein are retained by authors or by other copyright holders.
All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright.<br><br>
<form action="../../CVPR2019_search.py" method="post">
<input type="text" name="query">
<input type="submit" value="Search">
</form>
</div>
</div>
</div>
<div class="clear">
</div>
<div id="content">
<dl>
<dd>
<div id="papertitle">
Self-Supervised Learning via Conditional Motion Propagation</div>
<div id="authors">
<br><b><i>Xiaohang Zhan,  Xingang Pan,  Ziwei Liu,  Dahua Lin,  Chen Change Loy</i></b>; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1881-1889
</div><font size="5">
<br><b>Abstract</b>
</font>
<br><br><div id="abstract" >
Intelligent agent naturally learns from motion. Various self-supervised algorithms have leveraged the motion cues to learn effective visual representations. The hurdle here is that motion is both ambiguous and complex, rendering previous works either suffer from degraded learning efficacy, or resort to strong assumptions on object motions. In this work, we design a new learning-from-motion paradigm to bridge these gaps. Instead of explicitly modeling the motion probabilities, we design the pretext task as a conditional motion propagation problem. Given an input image and several sparse flow guidance on it, our framework seeks to recover the full-image motion. Compared to other alternatives, our framework has several appealing properties: (1) Using sparse flow guidance during training resolves the inherent motion ambiguity, and thus easing feature learning. (2) Solving the pretext task of conditional motion propagation encourages the emergence of kinematically-sound representations that poss greater expressive power. Extensive experiments demonstrate that our framework learns structural and coherent features; and achieves state-of-the-art self-supervision performance on several downstream tasks including semantic segmentation, instance segmentation and human parsing. Furthermore, our framework is successfully extended to several useful applications such as semi-automatic pixel-level annotation.</div>
<font size="5">
<br><b>Related Material</b>
</font>
<br><br>
[<a href="../../content_CVPR_2019/papers/Zhan_Self-Supervised_Learning_via_Conditional_Motion_Propagation_CVPR_2019_paper.pdf">pdf</a>]
[<a href="../../content_CVPR_2019/supplemental/Zhan_Self-Supervised_Learning_via_CVPR_2019_supplemental.pdf">supp</a>]
<div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
<div class="bibref">
@InProceedings{Zhan_2019_CVPR,<br>
author = {Zhan, Xiaohang and Pan, Xingang and Liu, Ziwei and Lin, Dahua and Loy, Chen Change},<br>
title = {Self-Supervised Learning via Conditional Motion Propagation},<br>
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
month = {June},<br>
year = {2019}<br>
}
</div>
</div>
</dd>
</dl>
</div>
</body>
</html>
