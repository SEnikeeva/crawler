<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CVPR 2018 Open Access Repository</title>
    <link rel="stylesheet" type="text/css" href="../../static/conf.css">
    <script type="text/javascript" src="../../static/jquery.js"></script>
    <meta name="citation_title" content="Boosting Self-Supervised Learning via Knowledge Transfer">
    <meta name="citation_author" content="Noroozi, Mehdi">
    <meta name="citation_author" content="Vinjimoor, Ananth">
    <meta name="citation_author" content="Favaro, Paolo">
    <meta name="citation_author" content="Pirsiavash, Hamed">
    <meta name="citation_publication_date" content="2018">
    <meta name="citation_conference_title"
          content="Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition">
    <meta name="citation_firstpage" content="9359">
    <meta name="citation_lastpage" content="9367">
    <meta name="citation_pdf_url"
          content="http://openaccess.thecvf.com/content_cvpr_2018/papers/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.pdf">
</head>
<body>
<div id="header">
    <div id="header_left">
        <a href="http://cvpr2018.thecvf.com"><img src="../../img/cvpr2018_logo.jpg" width="175" border="0"
                                                  alt="CVPR 2018"></a>
        <a href="http://www.cv-foundation.org/"><img src="../../img/cropped-cvf-s.jpg" width="175" height="112"
                                                     border="0" alt="CVF"></a>
    </div>
    <div id="header_right">
        <div id="header_title">
            <a href="http://cvpr2018.thecvf.com">CVPR 2018</a> <a href="/" class="a_monochrome">open access</a>
        </div>
        <div id="help">
            These CVPR 2018 papers are the Open Access versions, provided by the <a
                href="http://www.cv-foundation.org/">Computer Vision Foundation.</a><br> Except for the watermark, they
            are identical to the accepted versions; the final published version of the proceedings is available on IEEE
            Xplore.
        </div>
        <div id="disclaimer">
            This material is presented to ensure timely dissemination of scholarly and technical work.
            Copyright and all rights therein are retained by authors or by other copyright holders.
            All persons copying this information are expected to adhere to the terms and constraints invoked by each
            author's copyright.<br><br>
            <form action="../../CVPR2018_search.py" method="post">
                <input type="text" name="query">
                <input type="submit" value="Search">
            </form>
        </div>
    </div>
</div>
<div class="clear">
</div>
<div id="content">
    <dl>
        <dd>
            <div id="papertitle">
                Boosting Self-Supervised Learning via Knowledge Transfer
            </div>
            <div id="authors">
                <br><b><i>Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, Hamed Pirsiavash</i></b>; Proceedings of the
                IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 9359-9367
            </div>
            <font size="5">
                <br><b>Abstract</b>
            </font>
            <br><br>
            <div id="abstract">
                In self-supervised learning one trains a model to solve a so-called pretext task on a dataset without
                the need for human annotation. The main objective, however, is to transfer this model to a target domain
                and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the
                same model or parts thereof for both pretext and target tasks. In this paper, we present a novel
                framework for self-supervised learning that overcomes limitations in designing and comparing different
                tasks, models, and data domains. In particular, our framework decouples the structure of the
                self-supervised model from the final task-specific fine-tuned model. This allows us to: 1)
                quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper
                neural network models can learn better representations from the same pretext task; 3) transfer knowledge
                learned with a deep model to a shallower one and thus boost its learning. We use this framework to
                design a novel self-supervised task, which achieves state-of-the-art performance on the common
                benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. A surprising result is that
                our learned features shrink the mAP gap between models trained via self-supervised learning and
                supervised learning from $5.9$ to $2.6$ in object detection on PASCAL VOC 2007.
            </div>
            <font size="5">
                <br><b>Related Material</b>
            </font>
            <br><br>
            [<a href="../../content_cvpr_2018/papers/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.pdf">pdf</a>]
            [<a href="http://arxiv.org/abs/1805.00385v1">arXiv</a>]
            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                <div class="bibref">
                    @InProceedings{Noroozi_2018_CVPR,<br>
                    author = {Noroozi, Mehdi and Vinjimoor, Ananth and Favaro, Paolo and Pirsiavash, Hamed},<br>
                    title = {Boosting Self-Supervised Learning via Knowledge Transfer},<br>
                    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
                    (CVPR)},<br>
                    month = {June},<br>
                    year = {2018}<br>
                    }
                </div>
            </div>
        </dd>
    </dl>
</div>
</body>
</html>
