<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Search Engine Info --><title>Barlow Twins: Self-Supervised Learning via Redundancy Reduction</title>
    <meta name="description"
          content="Barlow Twins: Self-Supervised Learning via Redundancy ReductionJure Zbontar,&nbsp;Li Jing,&nbsp;Ishan Misra,&nbsp;Yann LeCun,&nbsp;Stephane DenySelf-supervis...">
    <!-- Solution from http://stackoverflow.com/questions/31593297/using-execcommand-javascript-to-copy-hidden-text-to-clipboard -->
    <script src="https://proceedings.mlr.press/v139/assets/js/copy_input.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://proceedings.mlr.press/v139/assets/js/download.js"></script>
    <meta name="citation_publisher" content="PMLR"/>
    <meta name="citation_title" content="Barlow Twins: Self-Supervised Learning via Redundancy Reduction"/>
    <meta name="citation_language" content="en"/>
    <meta name="citation_abstract_html_url" content="https://proceedings.mlr.press/v139/zbontar21a.html"/>
    <meta name="citation_pdf_url" content="http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf">
    <meta name="citation_firstpage" content="12310">
    <meta name="citation_lastpage" content="12320">
    <meta name="citation_author" content="Jure Zbontar">
    <meta name="citation_author" content="Li Jing">
    <meta name="citation_author" content="Ishan Misra">
    <meta name="citation_author" content="Yann LeCun">
    <meta name="citation_author" content="Stephane Deny">
    <meta name="citation_publication_date" content="2021/07/01">
    <meta name="citation_inbook_title" content="International Conference on Machine Learning"/>
    <meta name="citation_conference_title" content="International Conference on Machine Learning"/>
    <meta name="citation_issn" content="2640-3498">
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Barlow Twins: Self-Supervised Learning via Redundancy Reduction"/>
    <meta name="twitter:site" content="@MLResearchPress"/>
    <meta name="twitter:description"
          content="Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to lea..."/>
    <meta name="twitter:image" content="https://proceedings.mlr.press/v139/assets/images/logo-pmlr.png"/>
    <meta name="twitter:image:alt" content="PMLR Logo"/>
    <meta property="og:title" content="Barlow Twins: Self-Supervised Learning via Redundancy Reduction"/>
    <meta property="og:description"
          content="Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to dis..."/>
    <meta property="og:type" content="article"/>
    <meta property="article:published_time" content="2021-07-01T00:00:00+00:00">
    <meta property="og:url" content="https://proceedings.mlr.press/v139/zbontar21a.html"/>
    <meta property="og:image" content="https://proceedings.mlr.press/v139/assets/images/logo-pmlr.png"/>
    <meta property="og:site_name" content="PMLR"/><!-- Style Info -->
    <link rel="stylesheet" type="text/css" href="https://proceedings.mlr.press/v139/assets/css/pmlr.css"/>
    <style>.hero-text {
        color: #303030;
    }</style>

    <!-- Icon Info -->
    <link rel="shortcut icon" href="https://proceedings.mlr.press/v139/assets/images/favicon-pmlr.ico"
          type="image/x-icon">
    <link rel="icon" href="https://proceedings.mlr.press/v139/assets/images/favicon-pmlr.ico" type="image/x-icon">

    <!-- Feed Info -->
    <link type="application/atom+xml" rel="alternate" href="https://proceedings.mlr.press/v139/feed.xml"
          title="Proceedings of Machine Learning Research"/><!-- Scripting info -->
    <!-- Solution from http://stackoverflow.com/questions/31593297/using-execcommand-javascript-to-copy-hidden-text-to-clipboard -->
    <script src="https://proceedings.mlr.press/v139/assets/js/copy_input.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://proceedings.mlr.press/v139/assets/js/download.js"></script>

    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-92432422-1', 'auto');
        ga('send', 'pageview');

    </script>

    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$','$'], ['\\(', '\\)'] ],
      displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
      processEscapes: true,
    }
  });

    </script>
    <script async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML">
    </script>


    <!-- User custom header -->
</head>


<body>


<header class="site-header">
    <div class="wrapper">


        <div class="hero-image">
            <div class="hero-text">
                <a href="/" target="_top"><img src="/v139/assets/images/logo-pmlr.svg"
                                               alt="[International Conference on Machine Learning Logo]"></a>
                Proceedings of Machine Learning Research
            </div>
        </div>


        <nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
            <label for="nav-trigger">
  <span class="menu-icon"><svg viewBox="0 0 18 15" width="18px" height="15px">
  <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
</svg>
</span>
            </label>
            <div class="trigger"><a class="page-link" href="https://proceedings.mlr.press/v139/">Volume 139</a><a
                    class="page-link" href="http://www.jmlr.org/">JMLR</a>
                <a class="page-link" href="http://www.jmlr.org/mloss">MLOSS</a>
                <a class="page-link" href="/faq.html">FAQ</a>
                <a class="page-link" href="/spec.html">Submission Format</a>
                <a class="page-link" href="https://proceedings.mlr.press//v139/assets/rss/feed.xml">
                    <img src="https://proceedings.mlr.press/v139/assets/images/RSS.gif" class="rss" alt="RSS Feed">
                </a>
            </div>
        </nav>

    </div>
</header>


<main class="page-content" aria-label="Content">
    <div class="wrapper">


        <p style="text-align:right">[<a
                href="https://github.com/mlresearch/v139/edit/gh-pages/_posts/2021-07-01-zbontar21a.md" target="_blank"
                onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/mlresearch/v139/edit/gh-pages/_posts/2021-07-01-zbontar21a.md', 13);">edit</a>]
        </p>


        <article class="post-content">
            <h1>Barlow Twins: Self-Supervised Learning via Redundancy Reduction</h1><span class="authors">Jure Zbontar,&nbsp;Li Jing,&nbsp;Ishan Misra,&nbsp;Yann LeCun,&nbsp;Stephane Deny</span>
            <div id="info"><i>Proceedings of the 38th International Conference on Machine Learning</i>,&nbsp;PMLR
                139:12310-12320,&nbsp;2021.
            </div> <!-- info -->

            <h4>Abstract</h4>
            <div id="abstract" class="abstract">
                Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer
                vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to
                distortions of the input sample. However, a recurring issue with this approach is the existence of
                trivial constant solutions. Most current methods avoid such solutions by careful implementation details.
                We propose an objective function that naturally avoids collapse by measuring the cross-correlation
                matrix between the outputs of two identical networks fed with distorted versions of a sample, and making
                it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions
                of a sample to be similar, while minimizing the redundancy between the components of these vectors. The
                method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle
                applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry
                between the network twins such as a predictor network, gradient stopping, or a moving average on the
                weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins
                outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and
                is on par with current state of the art for ImageNet classification with a linear classifier head, and
                for transfer tasks of classification and object detection.
            </div>
            <h4>Cite this Paper</h4>
            <hr class="bibhr">

            <div class="bibbuttongroup">
                <div class="bibbuttontext"> BibTeX</div>
                <div class="codebox">
                    <code class="citecode" id="bibtex">
                        @InProceedings{pmlr-v139-zbontar21a,
                        title = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
                        author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
                        booktitle = {Proceedings of the 38th International Conference on Machine Learning},
                        pages = {12310--12320},
                        year = {2021},
                        editor = {Meila, Marina and Zhang, Tong},
                        volume = {139},
                        series = {Proceedings of Machine Learning Research},
                        month = {18--24 Jul},
                        publisher = {PMLR},
                        pdf = {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
                        url = {https://proceedings.mlr.press/v139/zbontar21a.html},
                        abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on
                        large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are
                        invariant to distortions of the input sample. However, a recurring issue with this approach is
                        the existence of trivial constant solutions. Most current methods avoid such solutions by
                        careful implementation details. We propose an objective function that naturally avoids collapse
                        by measuring the cross-correlation matrix between the outputs of two identical networks fed with
                        distorted versions of a sample, and making it as close to the identity matrix as possible. This
                        causes the embedding vectors of distorted versions of a sample to be similar, while minimizing
                        the redundancy between the components of these vectors. The method is called Barlow Twins, owing
                        to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical
                        networks. Barlow Twins does not require large batches nor asymmetry between the network twins
                        such as a predictor network, gradient stopping, or a moving average on the weight updates.
                        Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms
                        previous methods on ImageNet for semi-supervised classification in the low-data regime, and is
                        on par with current state of the art for ImageNet classification with a linear classifier head,
                        and for transfer tasks of classification and object detection.}
                        }
                    </code></div>
                <button class="bibbutton" id="button-bibtex1"
                        onclick="CopyToClipboard('bibtex');  ga('send', 'event', 'Ref Copy', 'CopyToClipboard', 'https://proceedings.mlr.press/v139/pmlr-v139-zbontar21a.bib', 15);">
                    Copy to Clipboard
                </button>
                <button class="bibbutton" id="button-bibtex2"
                        onclick="DownloadToFile('pmlr-v139-zbontar21a.bib', 'bibtex'); ga('send', 'event', 'Ref Downloads', 'Download', 'https://proceedings.mlr.press/v139/pmlr-v139-zbontar21a.bib', 16);">
                    Download
                </button>
            </div>


            <div class="bibbuttongroup">
                <div class="bibbuttontext"> Endnote</div>
                <div class="codebox">
                    <code class="citecode" id="endnote">%0 Conference Paper
                        %T Barlow Twins: Self-Supervised Learning via Redundancy Reduction
                        %A Jure Zbontar
                        %A Li Jing
                        %A Ishan Misra
                        %A Yann LeCun
                        %A Stephane Deny
                        %B Proceedings of the 38th International Conference on Machine Learning
                        %C Proceedings of Machine Learning Research
                        %D 2021
                        %E Marina Meila
                        %E Tong Zhang
                        %F pmlr-v139-zbontar21a
                        %I PMLR
                        %P 12310--12320
                        %U https://proceedings.mlr.press/v139/zbontar21a.html
                        %V 139
                        %X Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large
                        computer vision benchmarks. A successful approach to SSL is to learn embeddings which are
                        invariant to distortions of the input sample. However, a recurring issue with this approach is
                        the existence of trivial constant solutions. Most current methods avoid such solutions by
                        careful implementation details. We propose an objective function that naturally avoids collapse
                        by measuring the cross-correlation matrix between the outputs of two identical networks fed with
                        distorted versions of a sample, and making it as close to the identity matrix as possible. This
                        causes the embedding vectors of distorted versions of a sample to be similar, while minimizing
                        the redundancy between the components of these vectors. The method is called Barlow Twins, owing
                        to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical
                        networks. Barlow Twins does not require large batches nor asymmetry between the network twins
                        such as a predictor network, gradient stopping, or a moving average on the weight updates.
                        Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms
                        previous methods on ImageNet for semi-supervised classification in the low-data regime, and is
                        on par with current state of the art for ImageNet classification with a linear classifier head,
                        and for transfer tasks of classification and object detection.
                    </code></div>
                <button class="bibbutton" id="button-endnote1"
                        onclick="CopyToClipboard('endnote');  ga('send', 'event', 'Ref Copy', 'CopyToClipboard', 'https://proceedings.mlr.press/v139/pmlr-v139-zbontar21a.enw', 15);">
                    Copy to Clipboard
                </button>
                <button class="bibbutton" id="button-endnote2"
                        onclick="DownloadToFile('pmlr-v139-zbontar21a.enw', 'endnote'); ga('send', 'event', 'Ref Downloads', 'Download', 'https://proceedings.mlr.press/v139/pmlr-v139-zbontar21a.enw', 16);">
                    Download
                </button>
            </div>


            <div class="bibbuttongroup">
                <div class="bibbuttontext"> APA</div>
                <div class="codebox">
                    <code class="citecode" id="apa">
                        Zbontar, J., Jing, L., Misra, I., LeCun, Y. & Deny, S.. (2021). Barlow Twins: Self-Supervised
                        Learning via Redundancy Reduction. <i>Proceedings of the 38th International Conference on
                        Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 139:12310-12320
                        Available from https://proceedings.mlr.press/v139/zbontar21a.html.

                    </code></div>
                <button class="bibbutton" id="button-apa1"
                        onclick="CopyToClipboard('apa');  ga('send', 'event', 'Ref Copy', 'CopyToClipboard', 'https://proceedings.mlr.press/v139/pmlr-v139-zbontar21a.txt', 15);">
                    Copy to Clipboard
                </button>
                <button class="bibbutton" id="button-apa2"
                        onclick="DownloadToFile('pmlr-v139-zbontar21a.txt', 'apa'); ga('send', 'event', 'Ref Downloads', 'Download', 'https://proceedings.mlr.press/v139/pmlr-v139-zbontar21a.txt', 16);">
                    Download
                </button>
            </div>

            <hr class="bibhr">


            <h4>Related Material</h4>
            <div id="extras">
                <ul>
                    <li><a href="http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf" target="_blank"
                           onclick="ga('send', 'event', 'PDF Downloads', 'Download', 'http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf', 10);">Download
                        PDF</a></li>
                    <li><a href="http://proceedings.mlr.press/v139/zbontar21a/zbontar21a-supp.pdf" target="_blank"
                           onclick="ga('send', 'event', 'Extra Downloads', 'Download', 'http://proceedings.mlr.press/v139/zbontar21a/zbontar21a-supp.pdf', 12);">Supplementary
                        PDF</a></li>
                </ul>
            </div>


        </article>


    </div>
</main>


<footer class="site-footer">
    <div class="wrapper">
        <p>This site last compiled Mon, 17 Jan 2022 19:06:23 +0000</p>
        <div class="footer-left"><i><a href="https://github.com/mlresearch/v139">Github Account</a></i>
        </div>
        <div class="footer-right">Copyright &copy; <a href="https://proceedings.mlr.press">The authors and PMLR</a>
            2022.<a href="https://twitter.com/MLResearchPress"><i class="icon-twitter"></i>&nbsp;MLResearchPress</a>
        </div>
    </div>
</footer>


</body>

</html>
