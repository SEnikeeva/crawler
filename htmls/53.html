<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CVPR 2020 Open Access Repository</title>
    <link rel="stylesheet" type="text/css" href="../../static/conf.css">
    <script type="text/javascript" src="../../static/jquery.js"></script>
    <meta name="citation_title" content="Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos">
    <meta name="citation_author" content="Jakab, Tomas">
    <meta name="citation_author" content="Gupta, Ankush">
    <meta name="citation_author" content="Bilen, Hakan">
    <meta name="citation_author" content="Vedaldi, Andrea">
    <meta name="citation_publication_date" content="2020">
    <meta name="citation_conference_title"
          content="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
    <meta name="citation_firstpage" content="8787">
    <meta name="citation_lastpage" content="8797">
    <meta name="citation_pdf_url"
          content="http://openaccess.thecvf.com/content_CVPR_2020/papers/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.pdf">
</head>
<body>
<div id="header">
    <div id="header_left">
        <a href="http://cvpr2020.thecvf.com"><img src="../../img/cvpr2020_logo.png" width="175" border="0"
                                                  alt="CVPR 2020"></a>
        <a href="http://www.cv-foundation.org/"><img src="../../img/cropped-cvf-s.jpg" width="175" height="112"
                                                     border="0" alt="CVF"></a>
    </div>
    <div id="header_right">
        <div id="header_title">
            <a href="http://cvpr2020.thecvf.com">CVPR 2020</a> <a href="/" class="a_monochrome">open access</a>
        </div>
        <div id="help">
            These CVPR 2020 papers are the Open Access versions, provided by the <a
                href="http://www.cv-foundation.org/">Computer Vision Foundation.</a><br> Except for the watermark, they
            are identical to the accepted versions; the final published version of the proceedings is available on IEEE
            Xplore.
        </div>
        <div id="disclaimer">
            This material is presented to ensure timely dissemination of scholarly and technical work.
            Copyright and all rights therein are retained by authors or by other copyright holders.
            All persons copying this information are expected to adhere to the terms and constraints invoked by each
            author's copyright.<br><br>
            <form action="../../CVPR2020_search.py" method="post">
                <input type="text" name="query">
                <input type="submit" value="Search">
            </form>
        </div>
    </div>
</div>
<div class="clear">
</div>
<div id="content">
    <dl>
        <dd>
            <div id="papertitle">
                Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos
            </div>
            <div id="authors">
                <br><b><i>Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi</i></b>; Proceedings of the IEEE/CVF
                Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 8787-8797
            </div>
            <font size="5">
                <br><b>Abstract</b>
            </font>
            <br><br>
            <div id="abstract">
                We propose a new method for recognizing the pose of objects from a single image that for learning uses
                only unlabelled videos and a weak empirical prior on the object poses. Video frames differ primarily in
                the pose of the objects they contain, so our method distils the pose information by analyzing the
                differences between frames. The distillation uses a new dual representation of the geometry of objects
                as a set of 2D keypoints, and as a pictorial representation, i.e. a skeleton image. This has three
                benefits: (1) it provides a tight 'geometric bottleneck' which disentangles pose from appearance, (2) it
                can leverage powerful image-to-image translation networks to map between photometry and geometry, and
                (3) it allows to incorporate empirical pose priors in the learning process. The pose priors are obtained
                from unpaired data, such as from a different dataset or modality such as mocap, such that no annotated
                image is ever used in learning the pose recognition network. In standard benchmarks for pose recognition
                for humans and faces, our method achieves state-of-the-art performance among methods that do not require
                any labelled images for training. Project page: http://www.robots.ox.ac.uk/
                vgg/research/unsupervised_pose/
            </div>
            <font size="5">
                <br><b>Related Material</b>
            </font>
            <br><br>
            [<a href="../../content_CVPR_2020/papers/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.pdf">pdf</a>]
            [<a href="../../content_CVPR_2020/supplemental/Jakab_Self-Supervised_Learning_of_CVPR_2020_supplemental.pdf">supp</a>]
            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                <div class="bibref">
                    @InProceedings{Jakab_2020_CVPR,<br>
                    author = {Jakab, Tomas and Gupta, Ankush and Bilen, Hakan and Vedaldi, Andrea},<br>
                    title = {Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos},<br>
                    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
                    (CVPR)},<br>
                    month = {June},<br>
                    year = {2020}<br>
                    }
                </div>
            </div>
        </dd>
    </dl>
</div>
</body>
</html>
