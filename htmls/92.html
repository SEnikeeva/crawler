<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ECCV 2018 Open Access Repository</title>
    <link rel="stylesheet" type="text/css" href="../../static/conf.css">
    <script type="text/javascript" src="../../static/jquery.js"></script>
    <meta name="citation_title"
          content="ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo Systems">
    <meta name="citation_author" content="Zhang, Yinda">
    <meta name="citation_author" content="Khamis, Sameh">
    <meta name="citation_author" content="Rhemann, Christoph">
    <meta name="citation_author" content="Valentin, Julien">
    <meta name="citation_author" content="Kowdle, Adarsh">
    <meta name="citation_author" content="Tankovich, Vladimir">
    <meta name="citation_author" content="Schoenberg, Michael">
    <meta name="citation_author" content="Izadi, Shahram">
    <meta name="citation_author" content="Funkhouser, Thomas">
    <meta name="citation_author" content="Fanello, Sean">
    <meta name="citation_publication_date" content="2018">
    <meta name="citation_conference_title" content="Proceedings of the European Conference on Computer Vision (ECCV)">
    <meta name="citation_firstpage" content="784">
    <meta name="citation_lastpage" content="801">
    <meta name="citation_pdf_url"
          content="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yinda_Zhang_Active_Stereo_Net_ECCV_2018_paper.pdf">
</head>
<body>
<div id="header">
    <div id="header_left">
        <a href="https://eccv2018.org"><img src="../../img/eccv2018_logo.jpg" width="175" border="0"
                                            alt="ECCV 2018"></a>
        <a href="http://www.cv-foundation.org/"><img src="../../img/cropped-cvf-s.jpg" width="175" height="112"
                                                     border="0" alt="CVF"></a>
    </div>
    <div id="header_right">
        <div id="header_title">
            <a href="https://eccv2018.org">ECCV 2018</a> papers
        </div>
        <div id="help">
            The ECCV 2018 papers, provided here by the <a href="http://www.cv-foundation.org/">Computer Vision
            Foundation</a>, are the author-created versions. The content of the papers is identical to the content of
            the officially published ECCV 2018 LNCS version of the papers as available on SpringerLink: <a
                href="https://link.springer.com/conference/eccv">https://link.springer.com/conference/eccv</a>.
        </div>
        <div id="disclaimer">
            Intellectual property rights, copyright and all rights therein are retained by authors, by Springer as the
            publisher of the official ECCV 2018 proceedings or by other copyright holders. All persons copying this
            information are expected to adhere to the terms and constraints invoked by each author's copyright.<br><br>
            <form action="../../ECCV2018_search.py" method="post">
                <input type="text" name="query">
                <input type="submit" value="Search">
            </form>
        </div>
    </div>
</div>
<div class="clear">
</div>
<div id="content">
    <dl>
        <dd>
            <div id="papertitle">
                ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo Systems
            </div>
            <div id="authors">
                <br><b><i>Yinda Zhang, Sameh Khamis, Christoph Rhemann, Julien Valentin, Adarsh Kowdle, Vladimir
                Tankovich, Michael Schoenberg, Shahram Izadi, Thomas Funkhouser, Sean Fanello</i></b>; Proceedings of
                the European Conference on Computer Vision (ECCV), 2018, pp. 784-801
            </div>
            <font size="5">
                <br><b>Abstract</b>
            </font>
            <br><br>
            <div id="abstract">
                In this paper we present ActiveStereoNet, the first deep learning solution for active stereo systems.
                Due to the lack of ground truth, our method is fully self-supervised, yet it produces precise depth with
                a subpixel precision of 1/30th of a pixel; it does not suffer from the common over-smoothing issues of
                previous approaches; it preserves the edges; and it explicitly handles occlusions. We introduce a novel
                reconstruction loss that is more robust to noise and texture-less patches, and is invariant to
                illumination changes. The proposed loss is optimized using a window-based cost aggregation with an
                adaptive support weight scheme. This cost aggregation is edge-preserving and smooths the loss function,
                which is key to allow the network to reach compelling results. Finally we show how the task of
                predicting invalid regions, such as occlusions, can be trained end-to-end without ground-truth. This
                component is crucial to reduce blur and particularly improves predictions along depth discontinuities.
                Extensive quantitatively and qualitatively evaluations on real and synthetic data demonstrate state of
                the art results in many challenging scenes.
            </div>
            <font size="5">
                <br><b>Related Material</b>
            </font>
            <br><br>
            [<a href="../../content_ECCV_2018/papers/Yinda_Zhang_Active_Stereo_Net_ECCV_2018_paper.pdf">pdf</a>]
            [<a href="http://arxiv.org/abs/1807.06009v1">arXiv</a>]
            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                <div class="bibref">
                    @InProceedings{Zhang_2018_ECCV,<br>
                    author = {Zhang, Yinda and Khamis, Sameh and Rhemann, Christoph and Valentin, Julien and Kowdle,
                    Adarsh and Tankovich, Vladimir and Schoenberg, Michael and Izadi, Shahram and Funkhouser, Thomas and
                    Fanello, Sean},<br>
                    title = {ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo Systems},<br>
                    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},<br>
                    month = {September},<br>
                    year = {2018}<br>
                    }
                </div>
            </div>
        </dd>
    </dl>
</div>
</body>
</html>
